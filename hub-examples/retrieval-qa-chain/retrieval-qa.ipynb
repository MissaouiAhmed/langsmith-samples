{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6f4fd289-fd25-40b8-a503-f12c170d8b81",
      "metadata": {
        "id": "6f4fd289-fd25-40b8-a503-f12c170d8b81"
      },
      "source": [
        "# RetrievalQA Chain\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langsmith-cookbook/blob/main/hub-examples/retrieval-qa-chain/retrieval-qa.ipynb)\n",
        "\n",
        "Developing a production-grade LLM application requires many refinements, but tracking multiple versions of prompts, models, and other components can be cumbersome. The [LangChain Hub](https://smith.langchain.com/hub) offers a centralized registry to manage and version your LLM artifacts efficiently. It even lets you interact with these artifacts directly in the browser to facilitate easier collaboration with non-technical team members.\n",
        "\n",
        "[![Playground](https://github.com/langchain-ai/langsmith-cookbook/blob/main/hub-examples/retrieval-qa-chain/img/playground.png?raw=1)](https://smith.langchain.com/hub/rlm/rag-prompt/playground)\n",
        "\n",
        "In its initial release (08/05/2023), the hub is limited to prompt management, but we plan to add support for other artifacts soon.\n",
        "\n",
        "In this walkthrough, you will get started using the hub to manage prompts for a retrieval QA chain. You will go through the following steps:\n",
        "\n",
        "1. Load prompt from Hub\n",
        "2. Initialize Chain\n",
        "3. Run Chain\n",
        "4. Commit any new changes to the hub\n",
        "\n",
        "## Prerequsites\n",
        "\n",
        "#### a. Set up your LangSmith account\n",
        "\n",
        "While you can access public prompts without an account, pushing new prompts to the hub requires a LangSmith account. Create your account at https://smith.langchain.com and log in.\n",
        "\n",
        "Next, navigate to the [hub home](https://smith.langchain.com/hub). If you haven't already created a \"handle\", you will be prompted to do so. Your prompts and other artifacts will be saved within the namespace '<handle>/prompt-name', so choose one that you are comfortable sharing.\n",
        "\n",
        "#### b. Configure environment\n",
        "\n",
        "To use the hub, you'll want to use a recent version of LangChain and the `langchainhub` package. Install them with the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bfa759ba-d354-4e75-a158-bd1481ad82c1",
      "metadata": {
        "tags": [],
        "id": "bfa759ba-d354-4e75-a158-bd1481ad82c1"
      },
      "outputs": [],
      "source": [
        "%pip install -U langchain langchainhub --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8fa970d-c3b8-4288-a6f9-a946bcb75a43",
      "metadata": {
        "id": "f8fa970d-c3b8-4288-a6f9-a946bcb75a43"
      },
      "source": [
        "Finally, generate an API Key from your \"personal\" organization by navigating to the [LangSmith](https://smith.langchain.com) dashboard, and then set it in the cell below.\n",
        "\n",
        "**Note:** Currently (08/04/2023), only API keys from your 'personal' organization are supported! If you see a '403' error at any point in this walkthrough, please confirm you've set a valid API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "973cda39-bfed-4172-910d-395c8596ed89",
      "metadata": {
        "id": "973cda39-bfed-4172-910d-395c8596ed89"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Update with your API URL if using a hosted instance of Langsmith.\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "# Update with your API URL if using a hosted instance of Langsmith.\n",
        "os.environ[\"LANGCHAIN_HUB_API_URL\"] = \"https://api.hub.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_HUB_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6aea7ba-7e86-4958-9314-cfc563f3c08c",
      "metadata": {
        "id": "b6aea7ba-7e86-4958-9314-cfc563f3c08c"
      },
      "source": [
        "## 1. Load prompt\n",
        "\n",
        "Now it's time to load the prompt from the hub. We will use the `latest` version of [this retrieval QA prompt](https://smith.langchain.com/hub/rlm/rag-prompt) and later initialize the chain with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "88785ea3-ae64-49e1-b496-3970f5e6eb42",
      "metadata": {
        "tags": [],
        "id": "88785ea3-ae64-49e1-b496-3970f5e6eb42"
      },
      "outputs": [],
      "source": [
        "# RAG prompt\n",
        "from langchain import hub\n",
        "\n",
        "# Loads the latest version\n",
        "prompt = hub.pull(\"rlm/rag-prompt\", api_url=\"https://api.hub.langchain.com\")\n",
        "\n",
        "# To load a specific version, specify the version hash\n",
        "# prompt = hub.pull(\"rlm/rag-prompt:50442af1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8095257-bbbb-4146-bbc3-9371e163ee45",
      "metadata": {
        "id": "d8095257-bbbb-4146-bbc3-9371e163ee45"
      },
      "source": [
        "## 2. Create the QA chain\n",
        "\n",
        "Now that we've selected our prompt, initialize the chain.\n",
        " For this example, we will create a basic [RetrievalQA](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html?highlight=retrievalqa#langchain.chains.retrieval_qa.base.RetrievalQA) over a vectorstore retriever.\n",
        "\n",
        "Loading the data requires some amount of boilerplate, which we will run below.  While the specifics aren't important to this tutorial, you can learn more about Q&A in LangChain by visiting the [docs](https://python.langchain.com/docs/use_cases/question_answering/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "da0abdda-9766-4b08-8285-1cf803431656",
      "metadata": {
        "id": "da0abdda-9766-4b08-8285-1cf803431656",
        "outputId": "4836eb89-b3a2-44fb-bcba-8130c264da99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install chromadb langchain_community tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "274eec26-4b92-445f-9130-fda195566cee",
      "metadata": {
        "tags": [],
        "id": "274eec26-4b92-445f-9130-fda195566cee",
        "outputId": "31017e76-0f1f-49a8-eebf-fee93db6c5c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_openai'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-f052ca594851>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_text_splitters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#from langchain_community.chat_models import ChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#from langchain_community.embeddings import OpenAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_openai'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Load docs\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "#from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "data = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "all_splits = text_splitter.split_documents(data)\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9299312-ce29-4963-81f7-27efecbbd49c",
      "metadata": {
        "id": "e9299312-ce29-4963-81f7-27efecbbd49c"
      },
      "source": [
        "**Initialize the chain**. With the data added to the vectorstore, we can initialize the chain. We will\n",
        "pass the prompt in via the `chain_type_kwargs` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4df53934-6ad8-4729-a9e9-1730834d2f05",
      "metadata": {
        "tags": [],
        "id": "4df53934-6ad8-4729-a9e9-1730834d2f05"
      },
      "outputs": [],
      "source": [
        "# RetrievalQA\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm, retriever=vectorstore.as_retriever(), chain_type_kwargs={\"prompt\": prompt}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51aa87b5-3622-496b-b728-9522bf4eea1c",
      "metadata": {
        "id": "51aa87b5-3622-496b-b728-9522bf4eea1c"
      },
      "source": [
        "## 3. Run Chain\n",
        "\n",
        "Now that the chain is initialized, you can run it just like you would normally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5334b09b-a1a5-4c1f-ba96-ac6dff12e63e",
      "metadata": {
        "id": "5334b09b-a1a5-4c1f-ba96-ac6dff12e63e",
        "outputId": "e505449e-b37b-4875-f458-29315310da14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-563c10d15036>:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = qa_chain({\"query\": question})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The approaches to Task Decomposition include using LLM with simple prompting, task-specific instructions, and human inputs. Task decomposition involves breaking down large tasks into smaller subgoals for efficient handling of complex tasks and reflecting on past actions to improve future results. Challenges in long-term planning and task decomposition include planning over a lengthy history and effectively exploring the solution space.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "question = \"What are the approaches to Task Decomposition?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bc27733-b28d-4901-984b-c1c8422fcae6",
      "metadata": {
        "id": "7bc27733-b28d-4901-984b-c1c8422fcae6"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, you learned how to use the [hub](https://smith.langchain.com/hub?page=1) to manage prompts for a retrieval QA chain. The hub is a centralized location to manage, version, and share your prompts (and later, other artifacts).\n",
        "\n",
        "For more information, check out the [docs](https://docs.smith.langchain.com/hub/quickstart) or reach out to support@langchain.dev."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}